{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQrRLKAP5if7"
      },
      "source": [
        "# **Mount Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZoatMBf2F5k",
        "outputId": "947e18e0-7980-41dd-a587-f89992d502c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"from google.colab import drive\\ndrive.mount('/content/drive')\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''from google.colab import drive\n",
        "drive.mount('/content/drive')'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ItsVIByF4VBU"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/Shot Detection/Datasets/RAIDataset'\n",
        "video_dir = f\"{dataset_path}/videos\"\n",
        "frames_dir = f\"{dataset_path}/Frames\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnXn-V_-49f7",
        "outputId": "df927316-df55-4b28-d69b-701fbc82d25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['7.mp4', '6.mp4', '4.mp4']\n",
            "['scenes_8.txt', 'scenes_9.txt', 'scenes_10.txt']\n"
          ]
        }
      ],
      "source": [
        "# Checking the connection.\n",
        "import os\n",
        "print(os.listdir(video_dir)[:3])\n",
        "print(os.listdir(frames_dir)[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeNSpVbD5dLJ"
      },
      "source": [
        "# **Install Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNvelie95NFl",
        "outputId": "594897f0-d541-47ef-a9d6-161d61cd7607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.15.0)\n",
            "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.7.0.72)\n",
            "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.24.3)\n",
            "Requirement already satisfied: tensorflow-macos==2.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.23.4)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (69.5.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.29.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.3.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow opencv-python numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7lE7IS45oHY"
      },
      "source": [
        "# **Load Video Paths**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAydL_j65YM7",
        "outputId": "96727c32-450c-4fa3-ec1b-fb75e28b5102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/rishabhmathur/Documents/Development/Machine Learning/Projects/Shot Detection/Datasets/RAIDataset/videos/1.mp4', '/Users/rishabhmathur/Documents/Development/Machine Learning/Projects/Shot Detection/Datasets/RAIDataset/videos/10.mp4', '/Users/rishabhmathur/Documents/Development/Machine Learning/Projects/Shot Detection/Datasets/RAIDataset/videos/2.mp4']\n"
          ]
        }
      ],
      "source": [
        "def load_video_paths(video_dir):\n",
        "  videos = []\n",
        "  for file in os.listdir(video_dir):\n",
        "    if file.endswith(\".mp4\"):\n",
        "      videos.append(os.path.join(video_dir, file))\n",
        "  # Sort the videos as the order should match the annotations.\n",
        "  return sorted(videos)\n",
        "\n",
        "all_videos = load_video_paths(video_dir)\n",
        "print(all_videos[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGh35ZCL7pHC"
      },
      "source": [
        "# **Load and Parse Annotations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ6P8yFk6ZxB",
        "outputId": "f0e96ecb-25de-46d9-96a9-5406ccea676d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1, 717), (718, 1590), (1591, 2333)]\n"
          ]
        }
      ],
      "source": [
        "def load_annotations(ann_file):\n",
        "  scenes = []\n",
        "  with open(ann_file, 'r') as f:\n",
        "    for line in f:\n",
        "      start, end = map(int, line.strip().split())\n",
        "      scenes.append((start, end))\n",
        "\n",
        "  return scenes\n",
        "\n",
        "def get_annotaion_file(video_path):\n",
        "  video_num = os.path.splitext(os.path.basename(video_path))[0]\n",
        "  return os.path.join(frames_dir, f\"scenes_{video_num}.txt\")\n",
        "\n",
        "def is_shot_boundary(frame_idx, scenes):\n",
        "  for start, _ in scenes:\n",
        "    if frame_idx == start:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "# Example Usage: where we are printing first 3 scenes of a video.\n",
        "video_path = all_videos[0]\n",
        "ann_file = get_annotaion_file(video_path)\n",
        "scenes = load_annotations(ann_file)\n",
        "\n",
        "print(scenes[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYeU6grtpkTJ"
      },
      "source": [
        "# **Frame Extraction and Dataset Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqbxVvvrpOoY",
        "outputId": "d02d59c0-2b66-4e3b-da9c-3caa1258617b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /var/folders/ww/xtz3h8bj7qzdwgdg_pt4q_xr0000gn/T/ipykernel_2316/4203271683.py:42: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n",
            "WARNING:tensorflow:From /var/folders/ww/xtz3h8bj7qzdwgdg_pt4q_xr0000gn/T/ipykernel_2316/4203271683.py:42: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use output_signature instead\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def extract_frames(video_path, sample_rate=1, target_size=(224, 224)):\n",
        "  frames = []\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  frame_count = 0\n",
        "\n",
        "  while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "\n",
        "    if frame_count % sample_rate == 0:\n",
        "      frame = cv2.resize(frame, target_size)\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      frames.append(frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "  cap.release()\n",
        "  return np.array(frames) / 255.0\n",
        "\n",
        "def create_tf_dataset(video_paths, seq_length=3, batch_size=8):\n",
        "  def generator():\n",
        "    for video_path in video_paths:\n",
        "      frames = extract_frames(video_path)\n",
        "      ann_file = get_annotaion_file(video_path)\n",
        "      scenes = load_annotations(ann_file)\n",
        "\n",
        "      for i in range(len(frames) - seq_length):\n",
        "        # Extract sequence of frames from starting\n",
        "        seq = frames[i:i+seq_length]\n",
        "        mid_idx = i+seq_length // 2\n",
        "        label = 1 if is_shot_boundary(mid_idx+1, scenes) else 0\n",
        "        yield (seq, seq[seq_length // 2]), label\n",
        "\n",
        "  # Defining the output types and shapes for the dataset\n",
        "  output_types = ((tf.float32, tf.float32), tf.int32)\n",
        "  output_shapes = (( (seq_length, 224, 224, 3), (224, 224, 3)), ())\n",
        "  dataset = tf.data.Dataset.from_generator(generator, output_types, output_shapes)\n",
        "\n",
        "  return dataset.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "# Splitting videos into:-\n",
        "#  - Train\n",
        "#  - Test\n",
        "#  - Validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_videos, test_videos = train_test_split(all_videos, test_size=0.2, random_state=42)\n",
        "train_videos, val_videos = train_test_split(train_videos, test_size=0.25, random_state=42)\n",
        "\n",
        "# Creating datasets:-\n",
        "#   - Train\n",
        "#   - Test\n",
        "#   - Validation\n",
        "train_dataset = create_tf_dataset(train_videos)\n",
        "test_dataset = create_tf_dataset(test_videos)\n",
        "val_dataset = create_tf_dataset(val_videos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVQmO2OILMjc"
      },
      "source": [
        "# **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I9PWSPJLK_8L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 358s 17s/step - loss: 0.0585 - accuracy: 1.0000 - val_loss: 2.2443e-07 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x297b2ce50>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize and train the model\n",
        "class PatchEmbedd(tf.keras.layers.Layer):\n",
        "  def __init__(self, img_size, patch_size, embed_dim):\n",
        "    super().__init__()\n",
        "    self.proj = tf.keras.layers.Conv2D(embed_dim, patch_size, strides=patch_size)\n",
        "    self.flatten = tf.keras.layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.proj(x)\n",
        "    x = self.flatten(x)\n",
        "    return x\n",
        "\n",
        "def vit_model(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12):\n",
        "  inputs = tf.keras.Input(shape=(img_size, img_size, 3))\n",
        "  patches = PatchEmbedd(img_size, patch_size, embed_dim)(inputs)\n",
        "\n",
        "  num_patches = (img_size // patch_size) ** 2\n",
        "  positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "  pos_embed = tf.keras.layers.Embedding(input_dim=num_patches+1, output_dim=embed_dim)(positions)\n",
        "  x = patches + pos_embed\n",
        "\n",
        "  for _ in range(depth):\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x,x)\n",
        "    x = x + attn_output\n",
        "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    x = tf.keras.layers.Dense(units=embed_dim*4, activation=tf.nn.gelu)(x)\n",
        "    x = tf.keras.layers.Dense(units=embed_dim)(x)\n",
        "    x = x + patches\n",
        "\n",
        "  x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "def create_combined_model(img_size=224, lstm_units=256, seq_length=3):\n",
        "  vit = vit_model(img_size)\n",
        "  vit.trainable = False\n",
        "\n",
        "  frame_input = tf.keras.Input(shape=(img_size, img_size, 3))\n",
        "  frame_features = vit(frame_input)\n",
        "\n",
        "  sequence_input = tf.keras.Input(shape=(seq_length, img_size, img_size, 3))\n",
        "  x = tf.keras.layers.TimeDistributed(vit)(sequence_input)\n",
        "  x = tf.keras.layers.LSTM(lstm_units, return_sequences=True)(x)\n",
        "  x = tf.keras.layers.LSTM(lstm_units)(x)\n",
        "  x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = tf.keras.Model(inputs=[sequence_input, frame_input], outputs=x)\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "seq_length = 3\n",
        "model = create_combined_model(seq_length=seq_length)\n",
        "model.fit(train_dataset, epochs=1, validation_data=val_dataset, steps_per_epoch=20, validation_steps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AEmw7VKxYYwf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_dataset, steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_video\u001b[39m(video_path, model, threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_acc = model.evaluate(test_dataset, steps=10)\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "def split_video(video_path, model, threshold=0.5):\n",
        "    frames = extract_frames(video_path)\n",
        "    boundaries = [0]  # Always consider the first frame as a boundary\n",
        "\n",
        "    for i in range(len(frames) - seq_length):\n",
        "        seq = frames[i:i+seq_length]\n",
        "        middle_frame = seq[seq_length // 2]\n",
        "        pred = model.predict([np.expand_dims(seq, axis=0), np.expand_dims(middle_frame, axis=0)])\n",
        "        if pred[0][0] > threshold:\n",
        "            boundaries.append(i + seq_length // 2 + 1)  # +1 to match 1-indexing\n",
        "\n",
        "    return boundaries\n",
        "\n",
        "def save_video_parts(video_path, boundaries, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = None\n",
        "    current_part = 0\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count in boundaries:\n",
        "            if out is not None:\n",
        "                out.release()\n",
        "            current_part += 1\n",
        "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            out_path = f'{output_dir}/{video_name}_part_{current_part}.mp4'\n",
        "            out = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
        "\n",
        "        if out is not None:\n",
        "            out.write(frame)\n",
        "\n",
        "    cap.release()\n",
        "    if out is not None:\n",
        "        out.release()\n",
        "\n",
        "# Example: Split a video\n",
        "video_to_split = 'videos/6.mp4'\n",
        "detected_boundaries = split_video(video_to_split, model)\n",
        "output_dir = 'Shot Detection/video parts'  # Save parts back to Drive\n",
        "save_video_parts(video_to_split, detected_boundaries, output_dir)\n",
        "print(f\"Video parts saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
